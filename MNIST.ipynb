{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82136b6b",
   "metadata": {},
   "source": [
    "Buildng and Training Neural Networks on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998bf163",
   "metadata": {},
   "source": [
    "# 1. Build a Logistic Regression Model\n",
    "The most important part of the neural network is torch.nn - python neural network library.\n",
    "\n",
    "This library allows us to build neural networks by concatenating different types of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6533f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def model(input_size, num_classes):\n",
    "    return nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979bf529",
   "metadata": {},
   "source": [
    "Define a function named model, which returns a linear model (xW^{T}+b) by nn.Linear(input_size, num_classes)\n",
    "\n",
    "    input_size is the size of the row vector x\n",
    "\n",
    "    num_classes is the number of classes to be classified\n",
    "\n",
    "    This model includes parameters W and b.\n",
    "\n",
    "In MNIST, the size of all the images is 1*28*28, which can be re-arranged to a row vector x with size 784 so that input_size=784\n",
    "\n",
    "    num_classes=10 in MNIST, since there are 10 possible types of digits (from 0 to 9)\n",
    "\n",
    "    Therefore, the size of W is 10*784 and the size of b is 1*10 \n",
    "\n",
    "In summary, Linear model: xW^{T}+b\n",
    "\n",
    "    The size of W is num_classes*input_size\n",
    "\n",
    "    The size of b is 1*num_classes\n",
    "\n",
    "    The size of input x is 1*input_size (if batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f535dac",
   "metadata": {},
   "source": [
    "# 2. Load a Dataset\n",
    "The MNIST database of handwritten digits has a trainig dataset of 60,000 examples and a test dataset of 10,000 examples.\n",
    "\n",
    "The digits in this database are centered in a fixed-size image (1*28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6023047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4])\n",
      "torch.Size([2, 1, 28, 28])\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of the given image is tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# transforms images to a pytorch tensor\n",
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# load the set of training images\n",
    "trainset = torchvision.datasets.MNIST(root = './data', train = True, download = True, transform = MNIST_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 2, shuffle = True)\n",
    "\n",
    "# load the set of test images\n",
    "testset = torchvision.datasets.MNIST(root = './data', train = False, download = True, transform = MNIST_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 1, shuffle = False)\n",
    "\n",
    "# Plot some of images of digits\n",
    "def plot_images(images):\n",
    "    images_for_plot = images[0,0,:,:] # extract the 28*28 tensor from the tensor (image size)\n",
    "    print(images_for_plot.size())\n",
    "    plt.imshow(images_for_plot, cmap='gray')  # plot the image with colormaps='gray'\n",
    "    plt.show()\n",
    "\n",
    "for i, (images, labels) in enumerate(trainloader): # ith batch of images and labels\n",
    "    print(images.size())  # [batch_size, channel_size, image_size_x, image_size_y]\n",
    "    plot_images(images)\n",
    "    print('The label of the given image is',labels) \n",
    "    break  #force to stop the for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3b1c5",
   "metadata": {},
   "source": [
    "# 3. Train the Logistic Regression Model and Test the Training/Test Accuracy\n",
    "Using the model provided above, and using the loss function, which is definied by\n",
    "$$L(\\theta) :=\\frac{1}{N} \\sum_{j=1}^N\\ell(y_j, h(x_j; \\theta)).$$\n",
    "Here,N represents the size of dataset given, and $\\ell(y_j,h(x_j; \\theta))$ is the  general distance between real label and predicted label.\n",
    "\n",
    " $h(x_j; \\theta)$ is a probability distribution of data $x$.\n",
    " \n",
    " \n",
    " We are also able to find the training/test accuracy by dividing number of correct classification in a dataset by the total number of data in dataset.\n",
    " \n",
    " i.e., accuracy $ = \\frac{\\text{The number of correct classifications in a dataset}}{\\text{the total number of data in a dataset}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4bff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training accuracy: 89.54499816894531%, Test accuracy: 91.18000030517578%\n",
      "Epoch: 2, Training accuracy: 91.43999481201172%, Test accuracy: 91.68000030517578%\n",
      "Epoch: 3, Training accuracy: 91.76667022705078%, Test accuracy: 92.02999877929688%\n",
      "Epoch: 4, Training accuracy: 92.11500549316406%, Test accuracy: 92.31999969482422%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model\n",
    "def model(input_size, num_Classes):\n",
    "    return nn.Linear(input_size, num_classes)\n",
    "\n",
    "#initialize\n",
    "input_size = 784 # 1*28*28\n",
    "num_classes = 10 #mnist nums\n",
    "\n",
    "my_model = model(input_size, num_classes)\n",
    "\n",
    "# Cross Entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training algorithm\n",
    "    # Using Stochastic gradient descent method\n",
    "    # Step size of 0.1\n",
    "optimizer = optim.SGD(my_model.parameters(), lr = 0.2)\n",
    "\n",
    "# Dataset Loading\n",
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# Training\n",
    "    # Can change batch_size\n",
    "trainset = torchvision.datasets.MNIST(root = './data', train = True, download = True, transform = MNIST_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 30, shuffle = True)\n",
    "\n",
    "# Test\n",
    "    # Can change batch_size\n",
    "testset = torchvision.datasets.MNIST(root = '/data', train = False, download = True, transform = MNIST_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 20, shuffle = False)\n",
    "\n",
    "# Can change num_epochs\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    train_num_correct = 0\n",
    "    train_total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "       \n",
    "        images = images.reshape(images.size(0), 28*28) # [batch_size, 784]\n",
    "        \n",
    "        outputs = my_model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_p_max, train_predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_num_correct += (train_predicted == labels).sum()\n",
    "        train_accuracy = 100*(train_num_correct/train_total)\n",
    "\n",
    "    test_num_correct = 0\n",
    "    test_total = 0\n",
    "            \n",
    "    for i, (images, labels) in enumerate(testloader):\n",
    "        images = images.reshape(images.size(0), 28*28)\n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_num_correct += (predicted == labels).sum()\n",
    "        test_accuracy = 100*(test_num_correct/test_total)\n",
    "\n",
    "    print('Epoch: {}, Training accuracy: {}%, Test accuracy: {}%' .format(epoch+1, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c91a5",
   "metadata": {},
   "source": [
    "Epoch represents number of times an entire training dataset is used to train the neural network.\n",
    "\n",
    "In this case, we have four epoches, which means an entire dataset has been passed through the neural network four times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
